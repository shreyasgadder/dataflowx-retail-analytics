# Use Python 3.9 as base image
FROM python:3.9

# Set the working directory
WORKDIR /app

# Install necessary system dependencies
RUN apt-get update && apt-get install -y \
    netcat-openbsd \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install OpenJDK 11 via AdoptOpenJDK (AdoptOpenJDK is a reliable alternative for OpenJDK)
RUN mkdir -p /usr/share/man/man1 && \
    curl -L https://github.com/adoptium/temurin11-binaries/releases/download/jdk-11.0.18+10/OpenJDK11U-jdk_x64_linux_hotspot_11.0.18_10.tar.gz \
    | tar -xz -C /usr/local --strip-components=1
	
	
RUN apt-get update && apt-get install -y wget && \
	wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.2/spark-sql-kafka-0-10_2.12-3.5.2.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.2/spark-token-provider-kafka-0-10_2.12-3.5.2.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.5/snappy-java-1.1.10.5.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.0/postgresql-42.5.0.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.4.0/mongo-spark-connector_2.12-10.4.0.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/5.1.4/mongodb-driver-sync-5.1.4.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/mongodb/bson/5.1.4/bson-5.1.4.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/5.1.4/mongodb-driver-core-5.1.4.jar -P /opt/spark/jars/ && \
	wget https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/5.1.4/bson-record-codec-5.1.4.jar -P /opt/spark/jars/


# Set Java environment variable for PySpark
ENV JAVA_HOME=/usr/local
ENV PATH=$JAVA_HOME/bin:$PATH

# Copy scripts 
COPY pyspark/* .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Set environment variables for PySpark and Python
ENV PYTHONUNBUFFERED=1

EXPOSE 4045

# Default command
CMD ["bash", "-c", "trap 'exit 0' SIGTERM; while true; do sleep 1; done"]
